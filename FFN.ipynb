{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CheXpertFFN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "24_Gy4XAPh_m",
        "K4OkposPdFai",
        "eVCaypMrb9Ie"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "XX4lzebVPwT6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#X ray predictions - Baseline - Feedforward nets "
      ]
    },
    {
      "metadata": {
        "id": "4gyiizaSQHP5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from torchvision.transforms import ToTensor,Resize\n",
        "\n",
        "from PIL import Image\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import sys\n",
        "import csv\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as tfunc\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from PIL import Image\n",
        "import torch.nn.functional as func\n",
        "\n",
        "from sklearn.metrics.ranking import roc_auc_score\n",
        "import sklearn.metrics as metrics\n",
        "import random\n",
        "\n",
        "use_gpu = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NoWy52zEQd_Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Mount google drive"
      ]
    },
    {
      "metadata": {
        "id": "K52zUmD2QgEe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cf_fYW7HPztf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "baseFolder  = \"drive/My Drive/CheXpert Dataset/CheXpert-v1.0-small/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KyQIZO-UP1Wb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Utility functions for cleaning the data\n",
        "\n",
        "def cleanLabel(x):\n",
        "    \n",
        "    labelCount = 0    \n",
        "    if x.Pleural_Effusion == 1:\n",
        "        labelCount += 1\n",
        "    if x.Edema == 1:\n",
        "        labelCount += 1\n",
        "    if x.Cardiomegaly ==1:\n",
        "        labelCount += 1\n",
        "    if x.Pneumonia == 1:\n",
        "        labelCount += 1\n",
        "    return labelCount\n",
        "    \n",
        "    \n",
        "\n",
        "def getLabel(x):\n",
        "    \n",
        "    if x.Pleural_Effusion ==1:\n",
        "        return \"Pleural_Effusion\"\n",
        "    elif x.Edema == 1:\n",
        "        return \"Edema\"\n",
        "    elif x.Cardiomegaly==1:\n",
        "        return \"Cardiomegaly\"\n",
        "    elif x.Pneumonia == 1:\n",
        "        return \"Pneumonia\"\n",
        "    else:\n",
        "        return \"None\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_GO0ks8MP-hh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cols = ['Path',\n",
        " 'Sex',\n",
        " 'Age',\n",
        " 'View',\n",
        " 'AP/PA',\n",
        " 'No_Finding',\n",
        " 'Enlarged_Cardiomediastinum',\n",
        " 'Cardiomegaly',\n",
        " 'Lung_Opacity',\n",
        " 'Lung_Lesion',\n",
        " 'Edema',\n",
        " 'Consolidation',\n",
        " 'Pneumonia',\n",
        " 'Atelectasis',\n",
        " 'Pneumothorax',\n",
        " 'Pleural_Effusion',\n",
        " 'Pleural_Other',\n",
        " 'Fracture',\n",
        " 'Support_Devices']\n",
        "trainFile = pd.read_csv(os.path.join(baseFolder,'train.csv'), names = cols, header=0)\n",
        "validFile = pd.read_csv(os.path.join(baseFolder,'valid.csv'), names = cols, header=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-TQG2wyrRBB1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dataloader\n"
      ]
    },
    {
      "metadata": {
        "id": "FUUUnXctSVRG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labelMap = {\"Pleural_Effusion\":0, \"Edema\":1,\"Cardiomegaly\":2,\"Pneumonia\":3}\n",
        "\n",
        "def getLabelDf(x):\n",
        "    x = x[36:]          #To account for the extra \"././\" added before the Path variable\n",
        "    x = df.loc[df.Path == x] \n",
        "    return labelMap[x.label.values[0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "97Ms1n_rRAMv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LungDataset(Dataset):\n",
        "    \n",
        "\n",
        "    def __init__(self, csvFile, rootDir, transform = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            rootDir : Directory that has train, valid, train.csv and valid.csv\n",
        "            csvFile : train.csv or valid.csv\n",
        "        \"\"\"\n",
        "        \n",
        "        self.rootDir = rootDir\n",
        "        self.transform = transform\n",
        "        \n",
        "        cols = ['Path',\n",
        "                 'Sex',\n",
        "                 'Age',\n",
        "                 'View',\n",
        "                 'AP/PA',\n",
        "                 'No_Finding',\n",
        "                 'Enlarged_Cardiomediastinum',\n",
        "                 'Cardiomegaly',\n",
        "                 'Lung_Opacity',\n",
        "                 'Lung_Lesion',\n",
        "                 'Edema',\n",
        "                 'Consolidation',\n",
        "                 'Pneumonia',\n",
        "                 'Atelectasis',\n",
        "                 'Pneumothorax',\n",
        "                 'Pleural_Effusion',\n",
        "                 'Pleural_Other',\n",
        "                 'Fracture',\n",
        "                 'Support_Devices']\n",
        "        \n",
        "        self.df = pd.read_csv(os.path.join(rootDir,csvFile), names = cols, header=0)\n",
        "        \n",
        "        \n",
        "        #Modifying the path variable\n",
        "        self.df[\"Path\"] = self.df.Path.apply(lambda x : x.replace('CheXpert-v1.0-small',\"\")[1:])\n",
        "        \n",
        "        #retaining important vars\n",
        "        selectCols = ['Path',\"View\",'Sex',\"Pleural_Effusion\", \"Edema\",\"Cardiomegaly\",\"Pneumonia\"]\n",
        "        self.df = self.df[selectCols]\n",
        "        \n",
        "        self.df['isClean'] = self.df.apply(lambda x : cleanLabel(x), axis = 1)\n",
        "        #self.df[\"No\"] = (self.df.isClean == 0).astype(int)\n",
        "        #no_disease = self.df[self.df.isClean==0]\n",
        "        #Retaining only samples with 1 disease\n",
        "        self.df = self.df[self.df.isClean==1]\n",
        "        #self.df = pd.concat([self.df,no_disease])\n",
        "        \n",
        "        #Creating the label variable\n",
        "        self.df['label'] = self.df.apply(lambda x : labelMap[getLabel(x)], axis = 1)\n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        imgPath = os.path.join( self.rootDir, self.df.iloc[idx].Path)\n",
        "        image = Image.open(imgPath).convert('RGB')\n",
        "        \n",
        "        label = self.df.iloc[idx].label\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NKs_qVY5RGUX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Change train to valid if you want fast execution just for sanity checking the model (train image folders can be huge and can cause lame googledrive timout issues)\n",
        "trainDataset = LungDataset('train.csv', baseFolder, transforms.Compose([Resize((256,256)), ToTensor()]))\n",
        "validationDataset = LungDataset('valid.csv', baseFolder, transforms.Compose([Resize((256,256)), ToTensor()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3raXstr9RG-1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "e = next(iter(trainDataset))\n",
        "e[0].size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HCH4axUeS3o-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data_loader = DataLoader(trainDataset, batch_size= 64, shuffle = True, num_workers = 4)\n",
        "validation_loader = DataLoader(validationDataset, batch_size= 64, shuffle = True, num_workers = 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ZkmUn3URHHo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for image, label in train_data_loader:\n",
        "    print(label)\n",
        "    \n",
        "    break;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QjnhS5-BZMRI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IlZg-gCLZMzk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Logger"
      ]
    },
    {
      "metadata": {
        "id": "oqHb3SOhZSKW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MwXD9_-JO70r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = './logs'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "!if [ -f ngrok ] ; then echo \"Ngrok already installed\" ; else wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1 && unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1 ; fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "05j4zm8uO_Uv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9ZL1lY3UPBkT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print('Tensorboard Link: ' +str(json.load(sys.stdin)['tunnels'][0]['public_url']))\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H6qnFSapQU86",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Code referenced from https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.misc \n",
        "try:\n",
        "    from StringIO import StringIO  # Python 2.7\n",
        "except ImportError:\n",
        "    from io import BytesIO         # Python 3.x\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    \n",
        "    def __init__(self, log_dir):\n",
        "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
        "        self.writer = tf.summary.FileWriter(log_dir)\n",
        "\n",
        "    def scalar_summary(self, tag, value, step):\n",
        "        \"\"\"Log a scalar variable.\"\"\"\n",
        "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
        "        self.writer.add_summary(summary, step)\n",
        "\n",
        "    def image_summary(self, tag, images, step):\n",
        "        \"\"\"Log a list of images.\"\"\"\n",
        "\n",
        "        img_summaries = []\n",
        "        for i, img in enumerate(images):\n",
        "            # Write the image to a string\n",
        "            try:\n",
        "                s = StringIO()\n",
        "            except:\n",
        "                s = BytesIO()\n",
        "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
        "\n",
        "            # Create an Image object\n",
        "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
        "                                       height=img.shape[0],\n",
        "                                       width=img.shape[1])\n",
        "            # Create a Summary value\n",
        "            img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n",
        "\n",
        "        # Create and write Summary\n",
        "        summary = tf.Summary(value=img_summaries)\n",
        "        self.writer.add_summary(summary, step)\n",
        "        \n",
        "    def histo_summary(self, tag, values, step, bins=1000):\n",
        "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
        "\n",
        "        # Create a histogram using numpy\n",
        "        counts, bin_edges = np.histogram(values, bins=bins)\n",
        "\n",
        "        # Fill the fields of the histogram proto\n",
        "        hist = tf.HistogramProto()\n",
        "        hist.min = float(np.min(values))\n",
        "        hist.max = float(np.max(values))\n",
        "        hist.num = int(np.prod(values.shape))\n",
        "        hist.sum = float(np.sum(values))\n",
        "        hist.sum_squares = float(np.sum(values**2))\n",
        "\n",
        "        # Drop the start of the first bin\n",
        "        bin_edges = bin_edges[1:]\n",
        "\n",
        "        # Add bin edges and counts\n",
        "        for edge in bin_edges:\n",
        "            hist.bucket_limit.append(edge)\n",
        "        for c in counts:\n",
        "            hist.bucket.append(c)\n",
        "\n",
        "        # Create and write Summary\n",
        "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
        "        self.writer.add_summary(summary, step)\n",
        "        self.writer.flush()\n",
        "logger = Logger('./logs')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "24_Gy4XAPh_m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feedforward Network"
      ]
    },
    {
      "metadata": {
        "id": "6Wz1SRoDTdJy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cJlQn1pIQPTF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0')\n",
        "\n",
        "input_size = 196608\n",
        "hidden_size_1 = 1024\n",
        "hidden_size_2 = 512\n",
        "hidden_size_3 = 128\n",
        "num_classes = 4\n",
        "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size_1,hidden_size_2,hidden_size_3, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size_1) \n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden_size_2, hidden_size_3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc4 = nn.Linear(hidden_size_3, num_classes)\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc4(out)\n",
        "        return out\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size_1,hidden_size_2,hidden_size_3, num_classes).to(device)\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Giwch9Q5PhfG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for image, label in train_data_loader:\n",
        " #   print(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K4OkposPdFai",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the model\n"
      ]
    },
    {
      "metadata": {
        "id": "zVoJRqC-TjLU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 0.00003\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#criterion = nn.BCELoss(size_average = True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "epochs = 25\n",
        "# Train the model\n",
        "def train(net, optimizer, criterion, trainLoader, test_loader, epochs, size, model_name,plot):\n",
        "  model = net.to(device)\n",
        "  overall_step = 0;\n",
        "  for epoch in range(epochs):\n",
        "    loss_epoch = 0\n",
        "    for image, label in trainLoader:\n",
        "        image = image.reshape(-1, size).to(device)\n",
        "        image, label = image.to(device), label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(image)\n",
        "        loss = criterion(output, label)\n",
        "        loss_epoch = loss\n",
        "        loss.backward()\n",
        "        #print(loss)\n",
        "        optimizer.step()\n",
        "        \n",
        "        _, prediction = torch.max(output,1)\n",
        "        accuracy = (label == prediction.squeeze()).float().mean()\n",
        "        #print(\"Accuracy: \" + str(accuracy))\n",
        "        overall_step+=1\n",
        "        if plot:\n",
        "          info = { ('loss_' + model_name): loss.item(), ('accuracy_' + model_name): accuracy.item() }\n",
        "          for tag, value in info.items():\n",
        "            logger.scalar_summary(tag, value, overall_step+1)\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_lCMABMxVNRb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train(model, optimizer, criterion, train_data_loader, validation_loader, epochs, 196608 , 'ff_2', True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EZCB_SI8VOYo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eVCaypMrb9Ie",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluation of the model"
      ]
    },
    {
      "metadata": {
        "id": "ZKN7bCrKHQjQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "'''\n",
        "Convert to onehot encoded vector from a single integer\n",
        "\n",
        "'''\n",
        "\n",
        "def to_one_hot(y, n_dims=None):\n",
        "    y_tensor = y.data if isinstance(y, Variable) else y\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
        "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
        "    y_one_hot = y_one_hot.view(*y.shape, -1)\n",
        "    return Variable(y_one_hot) if isinstance(y, Variable) else y_one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jwsg3QmRHLpY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##Compute AUC for each of the classes\n",
        "def computeAUC (data, predicted, classCount):\n",
        "    auroc = []\n",
        "    data_np = data.cpu().numpy()\n",
        "    data_np_pred = predicted.cpu().numpy()\n",
        "    for i in range(classCount):\n",
        "        auroc.append(roc_auc_score(data_np[:, i], data_np_pred[:, i]))\n",
        "    return auroc\n",
        "\n",
        "##Compute the test accuracy and the AUC values\n",
        "def test(model, data_loader, class_count, class_names):   \n",
        "    out = torch.FloatTensor().cuda()\n",
        "    out_pred = torch.FloatTensor().cuda()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for image, label in data_loader:\n",
        "            image = image.reshape(-1, input_size).to(device)\n",
        "            target = to_one_hot(label.cuda(), 4).to(device)\n",
        "            out = torch.cat((out, target), 0).cuda()\n",
        "            out = model(image)\n",
        "            out_pred = torch.cat((out_pred, out), 0)\n",
        "        aurocClass = computeAUC(out, out_pred, 4)\n",
        "        aurocMean = np.array(aurocClass).mean()\n",
        "        print ('AUC: ', aurocMean)\n",
        "        for i in range (0, len(aurocClass)):\n",
        "            print (class_names[i], ' ', aurocClass[i])\n",
        "    return out, out_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G3VZcJomT6Dz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class_names = [\"Pleural_Effusion\", \"Edema\",\"Cardiomegaly\",\"Pneumonia\"]\n",
        "outGT1, outPRED1 = test(model, validation_loader, 4, class_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HcRPBA0PUBDs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(4):\n",
        "    fpr, tpr, threshold = metrics.roc_curve(out.cpu()[:,i], out_pred.cpu()[:,i])\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "    f = plt.subplot(2, 7, i+1)\n",
        "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "    fig_size[0] = 20\n",
        "    fig_size[1] = 20\n",
        "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
        "    plt.title('ROC for: ' + class_names[i])\n",
        "    plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5hBoPhKwdx5o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}