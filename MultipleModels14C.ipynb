{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MultipleModels14C.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "XX4lzebVPwT6"
      },
      "cell_type": "markdown",
      "source": [
        "# X Ray Predictions \n",
        "\n",
        " Using 2 separate models for frontal and lateral views and combining the results at a study level (patient id + study id combo)  using max, avg and weighted averages.\n",
        " "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4gyiizaSQHP5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import sys\n",
        "import csv\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from torchvision.transforms import ToTensor,Resize\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from sklearn.metrics.ranking import roc_auc_score\n",
        "import sklearn.metrics as metrics\n",
        "import random\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "use_gpu = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NoWy52zEQd_Y"
      },
      "cell_type": "markdown",
      "source": [
        "Mount google drive"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "K52zUmD2QgEe",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cf_fYW7HPztf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "baseFolder  = \"CheXpert-v1.0-small/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KyQIZO-UP1Wb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Utility functions for cleaning the data\n",
        "\n",
        "def cleanLabel(x):\n",
        "    \n",
        "    labelCount = 0    \n",
        "    if x.Pleural_Effusion == 1:\n",
        "        labelCount += 1\n",
        "    if x.Edema == 1:\n",
        "        labelCount += 1\n",
        "    if x.Cardiomegaly ==1:\n",
        "        labelCount += 1\n",
        "    if x.Pneumonia == 1:\n",
        "        labelCount += 1\n",
        "    return labelCount\n",
        "    \n",
        "    \n",
        "\n",
        "def getLabel(x):\n",
        "    \n",
        "    if x.Pleural_Effusion ==1:\n",
        "        return \"Pleural_Effusion\"\n",
        "    elif x.Edema == 1:\n",
        "        return \"Edema\"\n",
        "    elif x.Cardiomegaly==1:\n",
        "        return \"Cardiomegaly\"\n",
        "    elif x.Pneumonia == 1:\n",
        "        return \"Pneumonia\"\n",
        "    else:\n",
        "        return \"None\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_GO0ks8MP-hh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Dataframe columns\n",
        "cols = ['Path',\n",
        " 'Sex',\n",
        " 'Age',\n",
        " 'View',\n",
        " 'AP/PA',\n",
        " 'No_Finding',\n",
        " 'Enlarged_Cardiomediastinum',\n",
        " 'Cardiomegaly',\n",
        " 'Lung_Opacity',\n",
        " 'Lung_Lesion',\n",
        " 'Edema',\n",
        " 'Consolidation',\n",
        " 'Pneumonia',\n",
        " 'Atelectasis',\n",
        " 'Pneumothorax',\n",
        " 'Pleural_Effusion',\n",
        " 'Pleural_Other',\n",
        " 'Fracture',\n",
        " 'Support_Devices']\n",
        "\n",
        "pathFileTrain = baseFolder + 'train.csv'\n",
        "pathFileValid = baseFolder + 'valid.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-TQG2wyrRBB1"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataloader for using 14 classes\n",
        "\n",
        "Need to change uncertain labels (Trying 1s policy)\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FUUUnXctSVRG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#For 4 classes\n",
        "labelMap = {\"Pleural_Effusion\":0, \"Edema\":1,\"Cardiomegaly\":2,\"Pneumonia\":3}\n",
        "\n",
        "def getLabelDf(x):\n",
        "    x = x[36:]          #To account for the extra \"././\" added before the Path variable\n",
        "    x = df.loc[df.Path == x] \n",
        "    return labelMap[x.label.values[0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "97Ms1n_rRAMv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LungDataset(Dataset):\n",
        "    def __init__(self, baseFolder, file, transform=None, type=\"All\"):\n",
        "        image_files = []\n",
        "        labels = []\n",
        "\n",
        "        with open(file, \"r\") as f:\n",
        "            csvReader = csv.reader(f)\n",
        "            next(csvReader, None)\n",
        "            k=0\n",
        "            for line in csvReader:\n",
        "                k+=1\n",
        "                #print(line[0])\n",
        "                image_file= line[0]\n",
        "                if((type==\"All\") or (type==\"Frontal\" and image_file.endswith('frontal.jpg')) or (type == \"Lateral\" and image_file.endswith('lateral.jpg'))):\n",
        "                    #Create a 14 class label with 0s and 1s for the corresponding pathologies\n",
        "                    label = line[5:]\n",
        "\n",
        "                    #Handling uncertainity\n",
        "                    # TODO: Also try 0s for Us\n",
        "                    for i in range(14):\n",
        "                        if label[i]:\n",
        "                            a = float(label[i])\n",
        "                            if a == 1:\n",
        "                                label[i] = 1\n",
        "                            elif a == -1:\n",
        "                                label[i] = 1\n",
        "                            else:\n",
        "                                label[i] = 0\n",
        "                        else:\n",
        "                            label[i] = 0       \n",
        "                    #TODO: Change when running locally!    \n",
        "                    image_files.append(baseFolder + image_file)\n",
        "                    labels.append(label)\n",
        "                    \n",
        "        self.image_files = image_files\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.labels[index]\n",
        "        location = self.image_files[index]\n",
        "        image = Image.open(location).convert('RGB')\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, torch.FloatTensor(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yrCSVn6SNPzP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_size = 256\n",
        "input_transforms = transforms.Compose([\n",
        "            transforms.Resize((input_size, input_size)),\n",
        "            transforms.ColorJitter(brightness=.05, hue=.05, saturation=.05),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NKs_qVY5RGUX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#For drive:\n",
        "#trainDataset = LungDataset('drive/My Drive/CheXpert Dataset/',pathFileValid, transformSequence)  \n",
        "#Locally:\n",
        "trainFrontalDataset = LungDataset('',pathFileTrain, input_transforms, \"Frontal\")\n",
        "trainLateralDataset = LungDataset('',pathFileTrain, input_transforms, \"Lateral\")\n",
        "\n",
        "validationDataset = LungDataset('',pathFileValid, input_transforms)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "v-XxK6hl3J1I",
        "outputId": "0f7ac100-7b78-44e4-c2b5-08d316d59d26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print(validationDataset.__len__())\n",
        "print(trainFrontalDataset.__len__())\n",
        "print(trainLateralDataset.__len__())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "234\n",
            "191027\n",
            "32387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3raXstr9RG-1",
        "outputId": "83dc65de-725d-4510-bf2a-d45b0ae9ce30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "e = next(iter(trainFrontalDataset))\n",
        "e[0].size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 256, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HCH4axUeS3o-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_frontal_data_loader = DataLoader(trainFrontalDataset, batch_size= 16, shuffle = True)\n",
        "train_lateral_data_loader = DataLoader(trainLateralDataset, batch_size= 16, shuffle = True)\n",
        "\n",
        "validation_loader = DataLoader(validationDataset, batch_size= 16, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6ZkmUn3URHHo",
        "outputId": "f1be4988-eaa3-4753-b1d3-8db8228cf59e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "for image, label in train_lateral_data_loader:\n",
        "    print(label)\n",
        "    \n",
        "    break;"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "IlZg-gCLZMzk"
      },
      "cell_type": "markdown",
      "source": [
        "## Logger"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oqHb3SOhZSKW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MwXD9_-JO70r",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = './logs'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "!if [ -f ngrok ] ; then echo \"Ngrok already installed\" ; else wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1 && unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1 ; fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "05j4zm8uO_Uv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9ZL1lY3UPBkT",
        "outputId": "06120e43-afd5-4c66-848a-2978ccecc4a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print('Tensorboard Link: ' +str(json.load(sys.stdin)['tunnels'][0]['public_url']))\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorboard Link: https://b17f0923.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "H6qnFSapQU86",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Code referenced from https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.misc \n",
        "try:\n",
        "    from StringIO import StringIO  # Python 2.7\n",
        "except ImportError:\n",
        "    from io import BytesIO         # Python 3.x\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    \n",
        "    def __init__(self, log_dir):\n",
        "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
        "        self.writer = tf.summary.FileWriter(log_dir)\n",
        "\n",
        "    def scalar_summary(self, tag, value, step):\n",
        "        \"\"\"Log a scalar variable.\"\"\"\n",
        "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
        "        self.writer.add_summary(summary, step)\n",
        "\n",
        "    def image_summary(self, tag, images, step):\n",
        "        \"\"\"Log a list of images.\"\"\"\n",
        "\n",
        "        img_summaries = []\n",
        "        for i, img in enumerate(images):\n",
        "            # Write the image to a string\n",
        "            try:\n",
        "                s = StringIO()\n",
        "            except:\n",
        "                s = BytesIO()\n",
        "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
        "\n",
        "            # Create an Image object\n",
        "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
        "                                       height=img.shape[0],\n",
        "                                       width=img.shape[1])\n",
        "            # Create a Summary value\n",
        "            img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n",
        "\n",
        "        # Create and write Summary\n",
        "        summary = tf.Summary(value=img_summaries)\n",
        "        self.writer.add_summary(summary, step)\n",
        "        \n",
        "    def histo_summary(self, tag, values, step, bins=1000):\n",
        "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
        "\n",
        "        # Create a histogram using numpy\n",
        "        counts, bin_edges = np.histogram(values, bins=bins)\n",
        "\n",
        "        # Fill the fields of the histogram proto\n",
        "        hist = tf.HistogramProto()\n",
        "        hist.min = float(np.min(values))\n",
        "        hist.max = float(np.max(values))\n",
        "        hist.num = int(np.prod(values.shape))\n",
        "        hist.sum = float(np.sum(values))\n",
        "        hist.sum_squares = float(np.sum(values**2))\n",
        "\n",
        "        # Drop the start of the first bin\n",
        "        bin_edges = bin_edges[1:]\n",
        "\n",
        "        # Add bin edges and counts\n",
        "        for edge in bin_edges:\n",
        "            hist.bucket_limit.append(edge)\n",
        "        for c in counts:\n",
        "            hist.bucket.append(c)\n",
        "\n",
        "        # Create and write Summary\n",
        "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
        "        self.writer.add_summary(summary, step)\n",
        "        self.writer.flush()\n",
        "logger = Logger('./logs')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "24_Gy4XAPh_m"
      },
      "cell_type": "markdown",
      "source": [
        "## DenseNet for images - 14 class output"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6Wz1SRoDTdJy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JdT8coNP9QTN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zCY0XCSE9QTP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0')\n",
        "\n",
        "input_size = 196608\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.densenet = torchvision.models.densenet121(pretrained=True)\n",
        "        self.densenet.classifier = nn.Sequential(\n",
        "            nn.Linear(self.densenet.classifier.in_features, num_classes),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.densenet(x)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9F-5HhpN9QTQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(net, optimizer, criterion, train_loader, test_loader, epochs, size, model_name, plot):\n",
        "    model = net.to(device)\n",
        "    total_step = len(train_loader)\n",
        "    overall_step = 0\n",
        "    for epoch in range(epochs):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            # Move tensors to configured device\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            #Forward Pass\n",
        "            outputs = model(images)\n",
        "            \n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            #print(loss)\n",
        "            optimizer.step()\n",
        "            _, prediction = torch.max(outputs,1)\n",
        "            #print(outputs)\n",
        "            #print(labels)\n",
        "            #print(prediction)\n",
        "            ##total += labels.size(0)\n",
        "            #correct += (prediction==labels).sum().item()\n",
        "            #accuracy = (labels == prediction.squeeze()).float().mean()\n",
        "            overall_step+=1\n",
        "            del images\n",
        "            del labels\n",
        "            if (i+1) % 100 == 0:\n",
        "              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, i+1, total_step, loss.item()))\n",
        "            if plot:\n",
        "              info = { ('loss_' + model_name): loss.item()}\n",
        "\n",
        "              for tag, value in info.items():\n",
        "                logger.scalar_summary(tag, value, overall_step+1)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GojwdsVy95Pf"
      },
      "cell_type": "markdown",
      "source": [
        "### Create the frontal model (Densenet 121)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gjcOMfwj9QTW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "    # print(outputs)\n",
        "    # print(labels)    # print(outputs)\n",
        "    # print(labels)\n",
        "num_classes = 14\n",
        "learning_rate = 0.00003\n",
        "model_frontal = Net(num_classes).cuda()\n",
        "criterion = torch.nn.BCELoss(size_average = True)\n",
        "optimizer_frontal = torch.optim.Adam (model_frontal.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
        "epochs = 3\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2OrAUMek9u_t"
      },
      "cell_type": "markdown",
      "source": [
        "###Train the frontal network only on frontal images"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Nw8B8ewC9QTZ",
        "outputId": "e38583ac-3f5a-465c-bdc8-4fa2fc14a801",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Train the frontal model\n",
        "torch.cuda.empty_cache()\n",
        "train(model_frontal, optimizer_frontal, criterion, train_frontal_data_loader, validation_loader, epochs, input_size , 'front', True)\n",
        "\n",
        "torch.save(model_frontal.state_dict(), \"densenet_frontal.pt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/3], Step [100/11940], Loss: 0.4110\n",
            "Epoch [1/3], Step [200/11940], Loss: 0.3745\n",
            "Epoch [1/3], Step [300/11940], Loss: 0.4158\n",
            "Epoch [1/3], Step [400/11940], Loss: 0.4055\n",
            "Epoch [1/3], Step [500/11940], Loss: 0.3816\n",
            "Epoch [1/3], Step [600/11940], Loss: 0.3685\n",
            "Epoch [1/3], Step [700/11940], Loss: 0.3978\n",
            "Epoch [1/3], Step [800/11940], Loss: 0.3433\n",
            "Epoch [1/3], Step [900/11940], Loss: 0.3493\n",
            "Epoch [1/3], Step [1000/11940], Loss: 0.3362\n",
            "Epoch [1/3], Step [1100/11940], Loss: 0.3610\n",
            "Epoch [1/3], Step [1200/11940], Loss: 0.3810\n",
            "Epoch [1/3], Step [1300/11940], Loss: 0.3232\n",
            "Epoch [1/3], Step [1400/11940], Loss: 0.3824\n",
            "Epoch [1/3], Step [1500/11940], Loss: 0.4147\n",
            "Epoch [1/3], Step [1600/11940], Loss: 0.3305\n",
            "Epoch [1/3], Step [1700/11940], Loss: 0.3409\n",
            "Epoch [1/3], Step [1800/11940], Loss: 0.3539\n",
            "Epoch [1/3], Step [1900/11940], Loss: 0.3977\n",
            "Epoch [1/3], Step [2000/11940], Loss: 0.3883\n",
            "Epoch [1/3], Step [2100/11940], Loss: 0.3643\n",
            "Epoch [1/3], Step [2200/11940], Loss: 0.3668\n",
            "Epoch [1/3], Step [2300/11940], Loss: 0.4401\n",
            "Epoch [1/3], Step [2400/11940], Loss: 0.3453\n",
            "Epoch [1/3], Step [2500/11940], Loss: 0.3605\n",
            "Epoch [1/3], Step [2600/11940], Loss: 0.4383\n",
            "Epoch [1/3], Step [2700/11940], Loss: 0.3270\n",
            "Epoch [1/3], Step [2800/11940], Loss: 0.4033\n",
            "Epoch [1/3], Step [2900/11940], Loss: 0.3059\n",
            "Epoch [1/3], Step [3000/11940], Loss: 0.3923\n",
            "Epoch [1/3], Step [3100/11940], Loss: 0.3465\n",
            "Epoch [1/3], Step [3200/11940], Loss: 0.3660\n",
            "Epoch [1/3], Step [3300/11940], Loss: 0.3243\n",
            "Epoch [1/3], Step [3400/11940], Loss: 0.3302\n",
            "Epoch [1/3], Step [3500/11940], Loss: 0.3292\n",
            "Epoch [1/3], Step [3600/11940], Loss: 0.3747\n",
            "Epoch [1/3], Step [3700/11940], Loss: 0.4291\n",
            "Epoch [1/3], Step [3800/11940], Loss: 0.3381\n",
            "Epoch [1/3], Step [3900/11940], Loss: 0.3465\n",
            "Epoch [1/3], Step [4000/11940], Loss: 0.3651\n",
            "Epoch [1/3], Step [4100/11940], Loss: 0.3743\n",
            "Epoch [1/3], Step [4200/11940], Loss: 0.3733\n",
            "Epoch [1/3], Step [4300/11940], Loss: 0.3627\n",
            "Epoch [1/3], Step [4400/11940], Loss: 0.3251\n",
            "Epoch [1/3], Step [4500/11940], Loss: 0.4109\n",
            "Epoch [1/3], Step [4600/11940], Loss: 0.3564\n",
            "Epoch [1/3], Step [4700/11940], Loss: 0.3671\n",
            "Epoch [1/3], Step [4800/11940], Loss: 0.3298\n",
            "Epoch [1/3], Step [4900/11940], Loss: 0.2711\n",
            "Epoch [1/3], Step [5000/11940], Loss: 0.3727\n",
            "Epoch [1/3], Step [5100/11940], Loss: 0.4290\n",
            "Epoch [1/3], Step [5200/11940], Loss: 0.3686\n",
            "Epoch [1/3], Step [5300/11940], Loss: 0.3200\n",
            "Epoch [1/3], Step [5400/11940], Loss: 0.3156\n",
            "Epoch [1/3], Step [5500/11940], Loss: 0.3798\n",
            "Epoch [1/3], Step [5600/11940], Loss: 0.3418\n",
            "Epoch [1/3], Step [5700/11940], Loss: 0.3742\n",
            "Epoch [1/3], Step [5800/11940], Loss: 0.3433\n",
            "Epoch [1/3], Step [5900/11940], Loss: 0.2975\n",
            "Epoch [1/3], Step [6000/11940], Loss: 0.3753\n",
            "Epoch [1/3], Step [6100/11940], Loss: 0.3879\n",
            "Epoch [1/3], Step [6200/11940], Loss: 0.3207\n",
            "Epoch [1/3], Step [6300/11940], Loss: 0.3633\n",
            "Epoch [1/3], Step [6400/11940], Loss: 0.3377\n",
            "Epoch [1/3], Step [6500/11940], Loss: 0.3696\n",
            "Epoch [1/3], Step [6600/11940], Loss: 0.3721\n",
            "Epoch [1/3], Step [6700/11940], Loss: 0.3357\n",
            "Epoch [1/3], Step [6800/11940], Loss: 0.3591\n",
            "Epoch [1/3], Step [6900/11940], Loss: 0.3777\n",
            "Epoch [1/3], Step [7000/11940], Loss: 0.3843\n",
            "Epoch [1/3], Step [7100/11940], Loss: 0.3924\n",
            "Epoch [1/3], Step [7200/11940], Loss: 0.3917\n",
            "Epoch [1/3], Step [7300/11940], Loss: 0.4107\n",
            "Epoch [1/3], Step [7400/11940], Loss: 0.3129\n",
            "Epoch [1/3], Step [7500/11940], Loss: 0.3618\n",
            "Epoch [1/3], Step [7600/11940], Loss: 0.3444\n",
            "Epoch [1/3], Step [7700/11940], Loss: 0.3637\n",
            "Epoch [1/3], Step [7800/11940], Loss: 0.3418\n",
            "Epoch [1/3], Step [7900/11940], Loss: 0.2978\n",
            "Epoch [1/3], Step [8000/11940], Loss: 0.3370\n",
            "Epoch [1/3], Step [8100/11940], Loss: 0.3040\n",
            "Epoch [1/3], Step [8200/11940], Loss: 0.3554\n",
            "Epoch [1/3], Step [8300/11940], Loss: 0.3667\n",
            "Epoch [1/3], Step [8400/11940], Loss: 0.3668\n",
            "Epoch [1/3], Step [8500/11940], Loss: 0.3440\n",
            "Epoch [1/3], Step [8600/11940], Loss: 0.3562\n",
            "Epoch [1/3], Step [8700/11940], Loss: 0.3935\n",
            "Epoch [1/3], Step [8800/11940], Loss: 0.3181\n",
            "Epoch [1/3], Step [8900/11940], Loss: 0.4134\n",
            "Epoch [1/3], Step [9000/11940], Loss: 0.3440\n",
            "Epoch [1/3], Step [9100/11940], Loss: 0.3476\n",
            "Epoch [1/3], Step [9200/11940], Loss: 0.2929\n",
            "Epoch [1/3], Step [9300/11940], Loss: 0.3206\n",
            "Epoch [1/3], Step [9400/11940], Loss: 0.2856\n",
            "Epoch [1/3], Step [9500/11940], Loss: 0.3974\n",
            "Epoch [1/3], Step [9600/11940], Loss: 0.3532\n",
            "Epoch [1/3], Step [9700/11940], Loss: 0.3277\n",
            "Epoch [1/3], Step [9800/11940], Loss: 0.3173\n",
            "Epoch [1/3], Step [9900/11940], Loss: 0.3421\n",
            "Epoch [1/3], Step [10000/11940], Loss: 0.2742\n",
            "Epoch [1/3], Step [10100/11940], Loss: 0.3881\n",
            "Epoch [1/3], Step [10200/11940], Loss: 0.3413\n",
            "Epoch [1/3], Step [10300/11940], Loss: 0.3831\n",
            "Epoch [1/3], Step [10400/11940], Loss: 0.4180\n",
            "Epoch [1/3], Step [10500/11940], Loss: 0.2990\n",
            "Epoch [1/3], Step [10600/11940], Loss: 0.3402\n",
            "Epoch [1/3], Step [10700/11940], Loss: 0.3216\n",
            "Epoch [1/3], Step [10800/11940], Loss: 0.3341\n",
            "Epoch [1/3], Step [10900/11940], Loss: 0.3061\n",
            "Epoch [1/3], Step [11000/11940], Loss: 0.2808\n",
            "Epoch [1/3], Step [11100/11940], Loss: 0.4345\n",
            "Epoch [1/3], Step [11200/11940], Loss: 0.2883\n",
            "Epoch [1/3], Step [11300/11940], Loss: 0.2925\n",
            "Epoch [1/3], Step [11400/11940], Loss: 0.3826\n",
            "Epoch [1/3], Step [11500/11940], Loss: 0.3630\n",
            "Epoch [1/3], Step [11600/11940], Loss: 0.3292\n",
            "Epoch [1/3], Step [11700/11940], Loss: 0.3545\n",
            "Epoch [1/3], Step [11800/11940], Loss: 0.3644\n",
            "Epoch [1/3], Step [11900/11940], Loss: 0.3820\n",
            "Epoch [2/3], Step [100/11940], Loss: 0.3250\n",
            "Epoch [2/3], Step [200/11940], Loss: 0.3565\n",
            "Epoch [2/3], Step [300/11940], Loss: 0.3292\n",
            "Epoch [2/3], Step [400/11940], Loss: 0.3899\n",
            "Epoch [2/3], Step [500/11940], Loss: 0.2769\n",
            "Epoch [2/3], Step [600/11940], Loss: 0.3483\n",
            "Epoch [2/3], Step [700/11940], Loss: 0.3519\n",
            "Epoch [2/3], Step [800/11940], Loss: 0.3165\n",
            "Epoch [2/3], Step [900/11940], Loss: 0.3411\n",
            "Epoch [2/3], Step [1000/11940], Loss: 0.2987\n",
            "Epoch [2/3], Step [1100/11940], Loss: 0.4130\n",
            "Epoch [2/3], Step [1200/11940], Loss: 0.3150\n",
            "Epoch [2/3], Step [1300/11940], Loss: 0.4268\n",
            "Epoch [2/3], Step [1400/11940], Loss: 0.3763\n",
            "Epoch [2/3], Step [1500/11940], Loss: 0.3663\n",
            "Epoch [2/3], Step [1600/11940], Loss: 0.2992\n",
            "Epoch [2/3], Step [1700/11940], Loss: 0.3545\n",
            "Epoch [2/3], Step [1800/11940], Loss: 0.2995\n",
            "Epoch [2/3], Step [1900/11940], Loss: 0.3067\n",
            "Epoch [2/3], Step [2000/11940], Loss: 0.3241\n",
            "Epoch [2/3], Step [2100/11940], Loss: 0.3030\n",
            "Epoch [2/3], Step [2200/11940], Loss: 0.3058\n",
            "Epoch [2/3], Step [2300/11940], Loss: 0.3517\n",
            "Epoch [2/3], Step [2400/11940], Loss: 0.3383\n",
            "Epoch [2/3], Step [2500/11940], Loss: 0.2711\n",
            "Epoch [2/3], Step [2600/11940], Loss: 0.2929\n",
            "Epoch [2/3], Step [2700/11940], Loss: 0.3091\n",
            "Epoch [2/3], Step [2800/11940], Loss: 0.2529\n",
            "Epoch [2/3], Step [2900/11940], Loss: 0.3748\n",
            "Epoch [2/3], Step [3000/11940], Loss: 0.3483\n",
            "Epoch [2/3], Step [3100/11940], Loss: 0.3260\n",
            "Epoch [2/3], Step [3200/11940], Loss: 0.3405\n",
            "Epoch [2/3], Step [3300/11940], Loss: 0.2742\n",
            "Epoch [2/3], Step [3400/11940], Loss: 0.3007\n",
            "Epoch [2/3], Step [3500/11940], Loss: 0.3863\n",
            "Epoch [2/3], Step [3600/11940], Loss: 0.3875\n",
            "Epoch [2/3], Step [3700/11940], Loss: 0.3446\n",
            "Epoch [2/3], Step [3800/11940], Loss: 0.3487\n",
            "Epoch [2/3], Step [3900/11940], Loss: 0.2910\n",
            "Epoch [2/3], Step [4000/11940], Loss: 0.2983\n",
            "Epoch [2/3], Step [4100/11940], Loss: 0.3317\n",
            "Epoch [2/3], Step [4200/11940], Loss: 0.3085\n",
            "Epoch [2/3], Step [4300/11940], Loss: 0.3678\n",
            "Epoch [2/3], Step [4400/11940], Loss: 0.3376\n",
            "Epoch [2/3], Step [4500/11940], Loss: 0.3130\n",
            "Epoch [2/3], Step [4600/11940], Loss: 0.3167\n",
            "Epoch [2/3], Step [4700/11940], Loss: 0.3240\n",
            "Epoch [2/3], Step [4800/11940], Loss: 0.3354\n",
            "Epoch [2/3], Step [4900/11940], Loss: 0.3568\n",
            "Epoch [2/3], Step [5000/11940], Loss: 0.3491\n",
            "Epoch [2/3], Step [5100/11940], Loss: 0.3011\n",
            "Epoch [2/3], Step [5200/11940], Loss: 0.3711\n",
            "Epoch [2/3], Step [5300/11940], Loss: 0.2774\n",
            "Epoch [2/3], Step [5400/11940], Loss: 0.4268\n",
            "Epoch [2/3], Step [5500/11940], Loss: 0.3461\n",
            "Epoch [2/3], Step [5600/11940], Loss: 0.3192\n",
            "Epoch [2/3], Step [5700/11940], Loss: 0.3641\n",
            "Epoch [2/3], Step [5800/11940], Loss: 0.3940\n",
            "Epoch [2/3], Step [5900/11940], Loss: 0.2995\n",
            "Epoch [2/3], Step [6000/11940], Loss: 0.3391\n",
            "Epoch [2/3], Step [6100/11940], Loss: 0.3596\n",
            "Epoch [2/3], Step [6200/11940], Loss: 0.3265\n",
            "Epoch [2/3], Step [6300/11940], Loss: 0.3618\n",
            "Epoch [2/3], Step [6400/11940], Loss: 0.3827\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [2/3], Step [6500/11940], Loss: 0.3510\n",
            "Epoch [2/3], Step [6600/11940], Loss: 0.3401\n",
            "Epoch [2/3], Step [6700/11940], Loss: 0.3072\n",
            "Epoch [2/3], Step [6800/11940], Loss: 0.2966\n",
            "Epoch [2/3], Step [6900/11940], Loss: 0.3431\n",
            "Epoch [2/3], Step [7000/11940], Loss: 0.3045\n",
            "Epoch [2/3], Step [7100/11940], Loss: 0.3211\n",
            "Epoch [2/3], Step [7200/11940], Loss: 0.3293\n",
            "Epoch [2/3], Step [7300/11940], Loss: 0.3558\n",
            "Epoch [2/3], Step [7400/11940], Loss: 0.3262\n",
            "Epoch [2/3], Step [7500/11940], Loss: 0.3493\n",
            "Epoch [2/3], Step [7600/11940], Loss: 0.3223\n",
            "Epoch [2/3], Step [7700/11940], Loss: 0.2976\n",
            "Epoch [2/3], Step [7800/11940], Loss: 0.2616\n",
            "Epoch [2/3], Step [7900/11940], Loss: 0.3342\n",
            "Epoch [2/3], Step [8000/11940], Loss: 0.3241\n",
            "Epoch [2/3], Step [8100/11940], Loss: 0.3322\n",
            "Epoch [2/3], Step [8200/11940], Loss: 0.3194\n",
            "Epoch [2/3], Step [8300/11940], Loss: 0.3619\n",
            "Epoch [2/3], Step [8400/11940], Loss: 0.3106\n",
            "Epoch [2/3], Step [8500/11940], Loss: 0.3031\n",
            "Epoch [2/3], Step [8600/11940], Loss: 0.3275\n",
            "Epoch [2/3], Step [8700/11940], Loss: 0.3654\n",
            "Epoch [2/3], Step [8800/11940], Loss: 0.3790\n",
            "Epoch [2/3], Step [8900/11940], Loss: 0.3304\n",
            "Epoch [2/3], Step [9000/11940], Loss: 0.3365\n",
            "Epoch [2/3], Step [9100/11940], Loss: 0.4011\n",
            "Epoch [2/3], Step [9200/11940], Loss: 0.3475\n",
            "Epoch [2/3], Step [9300/11940], Loss: 0.3442\n",
            "Epoch [2/3], Step [9400/11940], Loss: 0.3286\n",
            "Epoch [2/3], Step [9500/11940], Loss: 0.3108\n",
            "Epoch [2/3], Step [9600/11940], Loss: 0.3997\n",
            "Epoch [2/3], Step [9700/11940], Loss: 0.3433\n",
            "Epoch [2/3], Step [9800/11940], Loss: 0.3946\n",
            "Epoch [2/3], Step [9900/11940], Loss: 0.3344\n",
            "Epoch [2/3], Step [10000/11940], Loss: 0.4137\n",
            "Epoch [2/3], Step [10100/11940], Loss: 0.3029\n",
            "Epoch [2/3], Step [10200/11940], Loss: 0.3183\n",
            "Epoch [2/3], Step [10300/11940], Loss: 0.2925\n",
            "Epoch [2/3], Step [10400/11940], Loss: 0.3019\n",
            "Epoch [2/3], Step [10500/11940], Loss: 0.3598\n",
            "Epoch [2/3], Step [10600/11940], Loss: 0.3048\n",
            "Epoch [2/3], Step [10700/11940], Loss: 0.3397\n",
            "Epoch [2/3], Step [10800/11940], Loss: 0.4053\n",
            "Epoch [2/3], Step [10900/11940], Loss: 0.4335\n",
            "Epoch [2/3], Step [11000/11940], Loss: 0.4160\n",
            "Epoch [2/3], Step [11100/11940], Loss: 0.2932\n",
            "Epoch [2/3], Step [11200/11940], Loss: 0.3562\n",
            "Epoch [2/3], Step [11300/11940], Loss: 0.3452\n",
            "Epoch [2/3], Step [11400/11940], Loss: 0.3504\n",
            "Epoch [2/3], Step [11500/11940], Loss: 0.3434\n",
            "Epoch [2/3], Step [11600/11940], Loss: 0.2860\n",
            "Epoch [2/3], Step [11700/11940], Loss: 0.2814\n",
            "Epoch [2/3], Step [11800/11940], Loss: 0.3038\n",
            "Epoch [2/3], Step [11900/11940], Loss: 0.4623\n",
            "Epoch [3/3], Step [100/11940], Loss: 0.3130\n",
            "Epoch [3/3], Step [200/11940], Loss: 0.3500\n",
            "Epoch [3/3], Step [300/11940], Loss: 0.3681\n",
            "Epoch [3/3], Step [400/11940], Loss: 0.3326\n",
            "Epoch [3/3], Step [500/11940], Loss: 0.3332\n",
            "Epoch [3/3], Step [600/11940], Loss: 0.2966\n",
            "Epoch [3/3], Step [700/11940], Loss: 0.3403\n",
            "Epoch [3/3], Step [800/11940], Loss: 0.4007\n",
            "Epoch [3/3], Step [900/11940], Loss: 0.3957\n",
            "Epoch [3/3], Step [1000/11940], Loss: 0.3391\n",
            "Epoch [3/3], Step [1100/11940], Loss: 0.3635\n",
            "Epoch [3/3], Step [1200/11940], Loss: 0.2968\n",
            "Epoch [3/3], Step [1300/11940], Loss: 0.3257\n",
            "Epoch [3/3], Step [1400/11940], Loss: 0.3447\n",
            "Epoch [3/3], Step [1500/11940], Loss: 0.3248\n",
            "Epoch [3/3], Step [1600/11940], Loss: 0.3051\n",
            "Epoch [3/3], Step [1700/11940], Loss: 0.3612\n",
            "Epoch [3/3], Step [1800/11940], Loss: 0.3150\n",
            "Epoch [3/3], Step [1900/11940], Loss: 0.3707\n",
            "Epoch [3/3], Step [2000/11940], Loss: 0.3113\n",
            "Epoch [3/3], Step [2100/11940], Loss: 0.3711\n",
            "Epoch [3/3], Step [2200/11940], Loss: 0.3169\n",
            "Epoch [3/3], Step [2300/11940], Loss: 0.3381\n",
            "Epoch [3/3], Step [2400/11940], Loss: 0.4432\n",
            "Epoch [3/3], Step [2500/11940], Loss: 0.3208\n",
            "Epoch [3/3], Step [2600/11940], Loss: 0.3186\n",
            "Epoch [3/3], Step [2700/11940], Loss: 0.3879\n",
            "Epoch [3/3], Step [2800/11940], Loss: 0.3361\n",
            "Epoch [3/3], Step [2900/11940], Loss: 0.2803\n",
            "Epoch [3/3], Step [3000/11940], Loss: 0.3453\n",
            "Epoch [3/3], Step [3100/11940], Loss: 0.3524\n",
            "Epoch [3/3], Step [3200/11940], Loss: 0.3188\n",
            "Epoch [3/3], Step [3300/11940], Loss: 0.4147\n",
            "Epoch [3/3], Step [3400/11940], Loss: 0.4233\n",
            "Epoch [3/3], Step [3500/11940], Loss: 0.3507\n",
            "Epoch [3/3], Step [3600/11940], Loss: 0.3052\n",
            "Epoch [3/3], Step [3700/11940], Loss: 0.3346\n",
            "Epoch [3/3], Step [3800/11940], Loss: 0.3866\n",
            "Epoch [3/3], Step [3900/11940], Loss: 0.3993\n",
            "Epoch [3/3], Step [4000/11940], Loss: 0.3529\n",
            "Epoch [3/3], Step [4100/11940], Loss: 0.3743\n",
            "Epoch [3/3], Step [4200/11940], Loss: 0.3104\n",
            "Epoch [3/3], Step [4300/11940], Loss: 0.3138\n",
            "Epoch [3/3], Step [4400/11940], Loss: 0.3779\n",
            "Epoch [3/3], Step [4500/11940], Loss: 0.3686\n",
            "Epoch [3/3], Step [4600/11940], Loss: 0.3036\n",
            "Epoch [3/3], Step [4700/11940], Loss: 0.2829\n",
            "Epoch [3/3], Step [4800/11940], Loss: 0.2920\n",
            "Epoch [3/3], Step [4900/11940], Loss: 0.4158\n",
            "Epoch [3/3], Step [5000/11940], Loss: 0.2872\n",
            "Epoch [3/3], Step [5100/11940], Loss: 0.3965\n",
            "Epoch [3/3], Step [5200/11940], Loss: 0.2717\n",
            "Epoch [3/3], Step [5300/11940], Loss: 0.3674\n",
            "Epoch [3/3], Step [5400/11940], Loss: 0.3334\n",
            "Epoch [3/3], Step [5500/11940], Loss: 0.3186\n",
            "Epoch [3/3], Step [5600/11940], Loss: 0.3598\n",
            "Epoch [3/3], Step [5700/11940], Loss: 0.3293\n",
            "Epoch [3/3], Step [5800/11940], Loss: 0.2653\n",
            "Epoch [3/3], Step [5900/11940], Loss: 0.3399\n",
            "Epoch [3/3], Step [6000/11940], Loss: 0.3509\n",
            "Epoch [3/3], Step [6100/11940], Loss: 0.3375\n",
            "Epoch [3/3], Step [6200/11940], Loss: 0.3517\n",
            "Epoch [3/3], Step [6300/11940], Loss: 0.3036\n",
            "Epoch [3/3], Step [6400/11940], Loss: 0.3449\n",
            "Epoch [3/3], Step [6500/11940], Loss: 0.3430\n",
            "Epoch [3/3], Step [6600/11940], Loss: 0.2931\n",
            "Epoch [3/3], Step [6700/11940], Loss: 0.2887\n",
            "Epoch [3/3], Step [6800/11940], Loss: 0.2973\n",
            "Epoch [3/3], Step [6900/11940], Loss: 0.3153\n",
            "Epoch [3/3], Step [7000/11940], Loss: 0.3500\n",
            "Epoch [3/3], Step [7100/11940], Loss: 0.4011\n",
            "Epoch [3/3], Step [7200/11940], Loss: 0.3168\n",
            "Epoch [3/3], Step [7300/11940], Loss: 0.3205\n",
            "Epoch [3/3], Step [7400/11940], Loss: 0.3274\n",
            "Epoch [3/3], Step [7500/11940], Loss: 0.3446\n",
            "Epoch [3/3], Step [7600/11940], Loss: 0.3024\n",
            "Epoch [3/3], Step [7700/11940], Loss: 0.3135\n",
            "Epoch [3/3], Step [7800/11940], Loss: 0.2945\n",
            "Epoch [3/3], Step [7900/11940], Loss: 0.3223\n",
            "Epoch [3/3], Step [8000/11940], Loss: 0.3349\n",
            "Epoch [3/3], Step [8100/11940], Loss: 0.3304\n",
            "Epoch [3/3], Step [8200/11940], Loss: 0.3335\n",
            "Epoch [3/3], Step [8300/11940], Loss: 0.3589\n",
            "Epoch [3/3], Step [8400/11940], Loss: 0.3204\n",
            "Epoch [3/3], Step [8500/11940], Loss: 0.3679\n",
            "Epoch [3/3], Step [8600/11940], Loss: 0.3479\n",
            "Epoch [3/3], Step [8700/11940], Loss: 0.3566\n",
            "Epoch [3/3], Step [8800/11940], Loss: 0.3593\n",
            "Epoch [3/3], Step [8900/11940], Loss: 0.3235\n",
            "Epoch [3/3], Step [9000/11940], Loss: 0.2980\n",
            "Epoch [3/3], Step [9100/11940], Loss: 0.3450\n",
            "Epoch [3/3], Step [9200/11940], Loss: 0.3154\n",
            "Epoch [3/3], Step [9300/11940], Loss: 0.3381\n",
            "Epoch [3/3], Step [9400/11940], Loss: 0.4288\n",
            "Epoch [3/3], Step [9500/11940], Loss: 0.3335\n",
            "Epoch [3/3], Step [9600/11940], Loss: 0.2965\n",
            "Epoch [3/3], Step [9700/11940], Loss: 0.3527\n",
            "Epoch [3/3], Step [9800/11940], Loss: 0.3288\n",
            "Epoch [3/3], Step [9900/11940], Loss: 0.4176\n",
            "Epoch [3/3], Step [10000/11940], Loss: 0.3711\n",
            "Epoch [3/3], Step [10100/11940], Loss: 0.4018\n",
            "Epoch [3/3], Step [10200/11940], Loss: 0.2443\n",
            "Epoch [3/3], Step [10300/11940], Loss: 0.3380\n",
            "Epoch [3/3], Step [10400/11940], Loss: 0.3525\n",
            "Epoch [3/3], Step [10500/11940], Loss: 0.4037\n",
            "Epoch [3/3], Step [10600/11940], Loss: 0.3359\n",
            "Epoch [3/3], Step [10700/11940], Loss: 0.2850\n",
            "Epoch [3/3], Step [10800/11940], Loss: 0.2984\n",
            "Epoch [3/3], Step [10900/11940], Loss: 0.3617\n",
            "Epoch [3/3], Step [11000/11940], Loss: 0.2879\n",
            "Epoch [3/3], Step [11100/11940], Loss: 0.3618\n",
            "Epoch [3/3], Step [11200/11940], Loss: 0.3729\n",
            "Epoch [3/3], Step [11300/11940], Loss: 0.3632\n",
            "Epoch [3/3], Step [11400/11940], Loss: 0.3567\n",
            "Epoch [3/3], Step [11500/11940], Loss: 0.3296\n",
            "Epoch [3/3], Step [11600/11940], Loss: 0.3602\n",
            "Epoch [3/3], Step [11700/11940], Loss: 0.3820\n",
            "Epoch [3/3], Step [11800/11940], Loss: 0.2818\n",
            "Epoch [3/3], Step [11900/11940], Loss: 0.3314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fmTI-76S-MoB"
      },
      "cell_type": "markdown",
      "source": [
        "### Create the lateral model "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1PajQzIC-MoC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model_lateral = Net(num_classes).cuda()\n",
        "optimizer_lateral = torch.optim.Adam (model_lateral.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
        "epochs = 6 # might want to train for more epochs (lesser lateral data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wb1EiMZx-MoI"
      },
      "cell_type": "markdown",
      "source": [
        "###Train the lateral network only on lateral images"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4uk37mJL-MoI",
        "outputId": "edac33bd-5c26-4ca9-e37f-d69697bdff64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "cell_type": "code",
      "source": [
        "## Train the frontal model\n",
        "\n",
        "train(model_lateral, optimizer_lateral, criterion, train_lateral_data_loader, validation_loader, epochs, input_size , 'lat', True)\n",
        "\n",
        "torch.save(model_frontal.state_dict(), \"densenet_lateral.pt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/6], Step [100/2025], Loss: 0.3977\n",
            "Epoch [1/6], Step [200/2025], Loss: 0.3147\n",
            "Epoch [1/6], Step [300/2025], Loss: 0.4457\n",
            "Epoch [1/6], Step [400/2025], Loss: 0.4452\n",
            "Epoch [1/6], Step [500/2025], Loss: 0.3604\n",
            "Epoch [1/6], Step [600/2025], Loss: 0.4595\n",
            "Epoch [1/6], Step [700/2025], Loss: 0.4552\n",
            "Epoch [1/6], Step [800/2025], Loss: 0.3470\n",
            "Epoch [1/6], Step [900/2025], Loss: 0.4617\n",
            "Epoch [1/6], Step [1000/2025], Loss: 0.3932\n",
            "Epoch [1/6], Step [1100/2025], Loss: 0.4277\n",
            "Epoch [1/6], Step [1200/2025], Loss: 0.3783\n",
            "Epoch [1/6], Step [1300/2025], Loss: 0.3066\n",
            "Epoch [1/6], Step [1400/2025], Loss: 0.3143\n",
            "Epoch [1/6], Step [1500/2025], Loss: 0.2954\n",
            "Epoch [1/6], Step [1600/2025], Loss: 0.3248\n",
            "Epoch [1/6], Step [1700/2025], Loss: 0.4219\n",
            "Epoch [1/6], Step [1800/2025], Loss: 0.3232\n",
            "Epoch [1/6], Step [1900/2025], Loss: 0.3765\n",
            "Epoch [1/6], Step [2000/2025], Loss: 0.3246\n",
            "Epoch [2/6], Step [100/2025], Loss: 0.3461\n",
            "Epoch [2/6], Step [200/2025], Loss: 0.3683\n",
            "Epoch [2/6], Step [300/2025], Loss: 0.4131\n",
            "Epoch [2/6], Step [400/2025], Loss: 0.3911\n",
            "Epoch [2/6], Step [500/2025], Loss: 0.3223\n",
            "Epoch [2/6], Step [600/2025], Loss: 0.3597\n",
            "Epoch [2/6], Step [700/2025], Loss: 0.3414\n",
            "Epoch [2/6], Step [800/2025], Loss: 0.3391\n",
            "Epoch [2/6], Step [900/2025], Loss: 0.3643\n",
            "Epoch [2/6], Step [1000/2025], Loss: 0.4441\n",
            "Epoch [2/6], Step [1100/2025], Loss: 0.3711\n",
            "Epoch [2/6], Step [1200/2025], Loss: 0.3709\n",
            "Epoch [2/6], Step [1300/2025], Loss: 0.3251\n",
            "Epoch [2/6], Step [1400/2025], Loss: 0.5384\n",
            "Epoch [2/6], Step [1500/2025], Loss: 0.4615\n",
            "Epoch [2/6], Step [1600/2025], Loss: 0.2971\n",
            "Epoch [2/6], Step [1700/2025], Loss: 0.3370\n",
            "Epoch [2/6], Step [1800/2025], Loss: 0.3559\n",
            "Epoch [2/6], Step [1900/2025], Loss: 0.3659\n",
            "Epoch [2/6], Step [2000/2025], Loss: 0.3222\n",
            "Epoch [3/6], Step [100/2025], Loss: 0.3027\n",
            "Epoch [3/6], Step [200/2025], Loss: 0.3389\n",
            "Epoch [3/6], Step [300/2025], Loss: 0.2466\n",
            "Epoch [3/6], Step [400/2025], Loss: 0.3251\n",
            "Epoch [3/6], Step [500/2025], Loss: 0.4203\n",
            "Epoch [3/6], Step [600/2025], Loss: 0.3190\n",
            "Epoch [3/6], Step [700/2025], Loss: 0.3712\n",
            "Epoch [3/6], Step [800/2025], Loss: 0.2851\n",
            "Epoch [3/6], Step [900/2025], Loss: 0.3581\n",
            "Epoch [3/6], Step [1000/2025], Loss: 0.2942\n",
            "Epoch [3/6], Step [1100/2025], Loss: 0.2937\n",
            "Epoch [3/6], Step [1200/2025], Loss: 0.3623\n",
            "Epoch [3/6], Step [1300/2025], Loss: 0.4062\n",
            "Epoch [3/6], Step [1400/2025], Loss: 0.3022\n",
            "Epoch [3/6], Step [1500/2025], Loss: 0.4268\n",
            "Epoch [3/6], Step [1600/2025], Loss: 0.4748\n",
            "Epoch [3/6], Step [1700/2025], Loss: 0.3359\n",
            "Epoch [3/6], Step [1800/2025], Loss: 0.2544\n",
            "Epoch [3/6], Step [1900/2025], Loss: 0.2475\n",
            "Epoch [3/6], Step [2000/2025], Loss: 0.3198\n",
            "Epoch [4/6], Step [100/2025], Loss: 0.2989\n",
            "Epoch [4/6], Step [200/2025], Loss: 0.4667\n",
            "Epoch [4/6], Step [300/2025], Loss: 0.2634\n",
            "Epoch [4/6], Step [400/2025], Loss: 0.3336\n",
            "Epoch [4/6], Step [500/2025], Loss: 0.3117\n",
            "Epoch [4/6], Step [600/2025], Loss: 0.3461\n",
            "Epoch [4/6], Step [700/2025], Loss: 0.3339\n",
            "Epoch [4/6], Step [800/2025], Loss: 0.2790\n",
            "Epoch [4/6], Step [900/2025], Loss: 0.3328\n",
            "Epoch [4/6], Step [1000/2025], Loss: 0.3141\n",
            "Epoch [4/6], Step [1100/2025], Loss: 0.2427\n",
            "Epoch [4/6], Step [1200/2025], Loss: 0.3313\n",
            "Epoch [4/6], Step [1300/2025], Loss: 0.2472\n",
            "Epoch [4/6], Step [1400/2025], Loss: 0.3375\n",
            "Epoch [4/6], Step [1500/2025], Loss: 0.3861\n",
            "Epoch [4/6], Step [1600/2025], Loss: 0.3135\n",
            "Epoch [4/6], Step [1700/2025], Loss: 0.3731\n",
            "Epoch [4/6], Step [1800/2025], Loss: 0.3334\n",
            "Epoch [4/6], Step [1900/2025], Loss: 0.3445\n",
            "Epoch [4/6], Step [2000/2025], Loss: 0.3563\n",
            "Epoch [5/6], Step [100/2025], Loss: 0.3387\n",
            "Epoch [5/6], Step [200/2025], Loss: 0.3249\n",
            "Epoch [5/6], Step [300/2025], Loss: 0.3766\n",
            "Epoch [5/6], Step [400/2025], Loss: 0.3581\n",
            "Epoch [5/6], Step [500/2025], Loss: 0.2818\n",
            "Epoch [5/6], Step [600/2025], Loss: 0.2771\n",
            "Epoch [5/6], Step [700/2025], Loss: 0.3268\n",
            "Epoch [5/6], Step [800/2025], Loss: 0.3468\n",
            "Epoch [5/6], Step [900/2025], Loss: 0.2309\n",
            "Epoch [5/6], Step [1000/2025], Loss: 0.2843\n",
            "Epoch [5/6], Step [1100/2025], Loss: 0.2774\n",
            "Epoch [5/6], Step [1200/2025], Loss: 0.3012\n",
            "Epoch [5/6], Step [1300/2025], Loss: 0.2672\n",
            "Epoch [5/6], Step [1400/2025], Loss: 0.3395\n",
            "Epoch [5/6], Step [1500/2025], Loss: 0.3159\n",
            "Epoch [5/6], Step [1600/2025], Loss: 0.3165\n",
            "Epoch [5/6], Step [1700/2025], Loss: 0.2640\n",
            "Epoch [5/6], Step [1800/2025], Loss: 0.3082\n",
            "Epoch [5/6], Step [1900/2025], Loss: 0.3793\n",
            "Epoch [5/6], Step [2000/2025], Loss: 0.2913\n",
            "Epoch [6/6], Step [100/2025], Loss: 0.2676\n",
            "Epoch [6/6], Step [200/2025], Loss: 0.3784\n",
            "Epoch [6/6], Step [300/2025], Loss: 0.2482\n",
            "Epoch [6/6], Step [400/2025], Loss: 0.3385\n",
            "Epoch [6/6], Step [500/2025], Loss: 0.2631\n",
            "Epoch [6/6], Step [600/2025], Loss: 0.2721\n",
            "Epoch [6/6], Step [700/2025], Loss: 0.2733\n",
            "Epoch [6/6], Step [800/2025], Loss: 0.3008\n",
            "Epoch [6/6], Step [900/2025], Loss: 0.2813\n",
            "Epoch [6/6], Step [1000/2025], Loss: 0.3237\n",
            "Epoch [6/6], Step [1100/2025], Loss: 0.2379\n",
            "Epoch [6/6], Step [1200/2025], Loss: 0.3227\n",
            "Epoch [6/6], Step [1300/2025], Loss: 0.2415\n",
            "Epoch [6/6], Step [1400/2025], Loss: 0.2901\n",
            "Epoch [6/6], Step [1500/2025], Loss: 0.3197\n",
            "Epoch [6/6], Step [1600/2025], Loss: 0.3104\n",
            "Epoch [6/6], Step [1700/2025], Loss: 0.3273\n",
            "Epoch [6/6], Step [1800/2025], Loss: 0.3229\n",
            "Epoch [6/6], Step [1900/2025], Loss: 0.2713\n",
            "Epoch [6/6], Step [2000/2025], Loss: 0.3480\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EZCB_SI8VOYo",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZKN7bCrKHQjQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "dlW4Mjbi_pmh"
      },
      "cell_type": "markdown",
      "source": [
        "### Predicting on testing set\n",
        "For each patient, compute output based on the frontal image and an output based on the lateral image (if present). Combine the results by traking the max of the the two predictions"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JbV8GJGHbNPl",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Loader for getting btoh frontal images and lateral images for a study\n",
        "\n",
        "test_df = pd.read_csv(pathFileValid)\n",
        "#test_df['root'] = test_df['path']\n",
        "x = test_df['Path'].str.split(\"/\", n = 4, expand = True)\n",
        "test_df['PatientStudyId'] = (x[2] + x[3])\n",
        "test_df['PatientStudyId'] = test_df['PatientStudyId'].astype(str)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PQxrMU4Rc8E_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "frontal_df = test_df[test_df['Frontal/Lateral'] == 'Frontal']\n",
        "lateral_df = test_df[test_df['Frontal/Lateral'] == 'Lateral']\n",
        "combined_df = pd.merge(frontal_df, lateral_df, on='PatientStudyId', how='outer', suffixes=('_frontal', '_lateral'))\n",
        "combined_df = combined_df[['Path_frontal', 'Path_lateral']]\n",
        "\n",
        "combined_df = combined_df.where((pd.notnull(combined_df)), None)\n",
        "front_lateral_dict = combined_df.set_index('Path_frontal').to_dict()\n",
        "lateral_front_dict = combined_df.set_index('Path_lateral').to_dict()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6Mp-h6RflUQ7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BDKWOYJr4f0S"
      },
      "cell_type": "markdown",
      "source": [
        "#### Create test dataset given the combined front and lateral image maps"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9xZ-H8l2Msaf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "front_lateral_dict = front_lateral_dict['Path_lateral']\n",
        "lateral_front_dict = lateral_front_dict['Path_frontal']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pFDNc8QVoWV8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "class CombinedTestDataset(Dataset):\n",
        "    def __init__(self, baseFolder, file,  front_lat_dict, lat_front_dict, transform=None):\n",
        "        frontal_image_files = []\n",
        "        lateral_image_files = []\n",
        "        labels = []\n",
        "        hasFrontal = []\n",
        "        hasLateral = []\n",
        "        with open(file, \"r\") as f:\n",
        "            csvReader = csv.reader(f)\n",
        "            next(csvReader, None)\n",
        "            k=0\n",
        "            for line in csvReader:\n",
        "                k+=1\n",
        "                #print(line[0])\n",
        "                \n",
        "                image_file= line[0]\n",
        "                if(line[3] == 'Frontal'):\n",
        "                    lateral_img_path = front_lat_dict[image_file]\n",
        "                    frontal_image_files.append(baseFolder + image_file)\n",
        "                    hasFrontal.append(True)\n",
        "                    if(lateral_img_path is not None):\n",
        "                        lateral_image_files.append(baseFolder + lateral_img_path)\n",
        "                        hasLateral.append(True)\n",
        "                    else:\n",
        "                        lateral_image_files.append(None)\n",
        "                        hasLateral.append(False)\n",
        "                \n",
        "                else:\n",
        "                    frontal_image_path =  lat_front_dict[image_file]\n",
        "                    lateral_image_files.append(baseFolder + image_file)\n",
        "                    hasLateral.append(True)\n",
        "                    if(frontal_image_path is not None):\n",
        "                        hasFrontal.append(True)\n",
        "                        frontal_image_files.append(baseFolder + frontal_image_path)\n",
        "                    else:\n",
        "                        hasFrontal.append(False)\n",
        "                        frontal_image_files.append(None)\n",
        "                \n",
        "                #Create a 14 class label with 0s and 1s for the corresponding pathologies\n",
        "                label = line[5:]\n",
        "\n",
        "                #Handling uncertainity\n",
        "                # TODO: Also try 0s for Us\n",
        "                for i in range(14):\n",
        "                    if label[i]:\n",
        "                        a = float(label[i])\n",
        "                        if a == 1:\n",
        "                            label[i] = 1\n",
        "                        elif a == -1:\n",
        "                            label[i] = 1\n",
        "                        else:\n",
        "                            label[i] = 0\n",
        "                    else:\n",
        "                        label[i] = 0       \n",
        "                #TODO: Change when running locally!    \n",
        "                labels.append(label)\n",
        "                    \n",
        "        self.frontal_image_files = frontal_image_files\n",
        "        self.lateral_image_files = lateral_image_files\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.hasFrontal = hasFrontal\n",
        "        self.hasLateral = hasLateral\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        label = self.labels[index]\n",
        "        frontal_location = self.frontal_image_files[index]\n",
        "        hasF = self.hasFrontal[index]\n",
        "        hasL = self.hasLateral[index]\n",
        "        \n",
        "        frontal_image = None\n",
        "        if(hasF):\n",
        "            frontal_image = Image.open(frontal_location).convert('RGB')\n",
        "            if self.transform is not None:\n",
        "                frontal_image = self.transform(frontal_image)\n",
        "                \n",
        "        \n",
        "        lateral_location = self.lateral_image_files[index]\n",
        "        lateral_image = None\n",
        "        if(hasL):\n",
        "            lateral_image = Image.open(lateral_location).convert('RGB')\n",
        "            if self.transform is not None:\n",
        "                lateral_image = self.transform(lateral_image)\n",
        "        hasFTensor = torch.ones([1,1])\n",
        "        hasLTensor = torch.ones([1,1])\n",
        "        if(frontal_image is None):\n",
        "            hasFTensor = torch.zeros([1,1])\n",
        "            frontal_image = lateral_image\n",
        "        elif(lateral_image is None):\n",
        "            hasLTensor = torch.zeros([1,1])\n",
        "            lateral_image = frontal_image\n",
        "        \n",
        "        \n",
        "        return frontal_image, lateral_image, torch.FloatTensor(label), hasFTensor, hasLTensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.frontal_image_files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1suLFnNSfOqw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "testDataset = CombinedTestDataset('',pathFileValid, front_lateral_dict, lateral_front_dict, transformSequence)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5Ywa0Cx88J3f",
        "outputId": "c2c8b0b6-c145-4b4f-ef0e-357830f59646",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "e = next(iter(testDataset))\n",
        "e[0].size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 256, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "d6753WQT4qxZ"
      },
      "cell_type": "markdown",
      "source": [
        "#### Create the test dataloader\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2H1r4_xJ4pkO",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(testDataset, batch_size= 1, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eU6SY6tX5PhW",
        "outputId": "97ec45ba-fb2a-44b1-b356-8c077b5cfc9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "for i, (images_f, images_l, labels, has_f, has_l) in enumerate(test_loader):\n",
        "    print(images_f.size())\n",
        "    print(images_l.size())\n",
        "    print(has_f.size())\n",
        "    \n",
        "    break;"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jwsg3QmRHLpY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def class_roc_auc_combined (data, predicted, class_count = 14):\n",
        "\n",
        "    auroc = []\n",
        "\n",
        "    data_np = data.cpu().numpy()\n",
        "    data_np_pred = predicted.cpu().numpy()\n",
        "\n",
        "    for i in range(class_count):\n",
        "        auroc.append(roc_auc_score(data_np[:, i], data_np_pred[:, i]))\n",
        "    return auroc\n",
        "\n",
        "\n",
        "\n",
        "def test_combined(modelFrontal, modelLateral, test_loader, class_count, class_names):   \n",
        "    out = torch.FloatTensor().cuda()\n",
        "    out_pred = torch.FloatTensor().cuda()\n",
        "    modelFrontal.eval()\n",
        "    modelLateral.eval()\n",
        "    with torch.no_grad():\n",
        "        for imageF, imageL, label, hasF, hasL in test_loader:\n",
        "            imageF = imageF.to(device)\n",
        "            imageL = imageL.to(device)\n",
        "            target = label.cuda()\n",
        "            out = torch.cat((out, target), 0).cuda()\n",
        "            hasFSum = hasF.float().sum().data.item()\n",
        "            hasLSum = hasL.float().sum().data.item()\n",
        "            if hasFSum != 0 and hasLSum != 0:\n",
        "                outF = modelFrontal(imageF)\n",
        "                outL = modelLateral(imageL)\n",
        "                outV = torch.max(outF, outL)\n",
        "            elif hasLSum != 0:\n",
        "                outV = modelLateral(imageL)\n",
        "            else:\n",
        "                outV = modelFrontal(imageF)\n",
        "                \n",
        "            outV = outV.cuda()\n",
        "            out_pred = torch.cat((out_pred, outV), 0).cuda()\n",
        "    roc_auc_classes = class_roc_auc_combined(out, out_pred, class_count)\n",
        "    auroc_mean = np.array(roc_auc_classes).mean()\n",
        "\n",
        "    print ('AUROC mean ', auroc_mean)\n",
        "\n",
        "    for i in range (class_count):\n",
        "        print (class_names[i], ' ', roc_auc_classes[i])\n",
        "    return out, out_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MFFfAktenGq0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Class names\n",
        "class_names = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', \n",
        "               'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', \n",
        "               'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']\n",
        "out, out_pred = test_combined(model_frontal, model_lateral, test_loader, class_names.length, class_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "G3VZcJomT6Dz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HcRPBA0PUBDs",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "viHlSMv_gxRX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vsZkuhtQp2X-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}